{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "wLxVTUfjQfAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7sUwa98Vrwt",
        "outputId": "0cc6a1ff-fd23-433c-c5aa-21cee4815e0f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "HH5NufBCQAF0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense, Dropout, LayerNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import img_to_array, load_img\n",
        "from tensorflow import keras\n",
        "%load_ext tensorboard\n",
        "\n",
        "\n",
        "import os"
      ],
      "metadata": {
        "trusted": true,
        "id": "VWqKYVzxP-7k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a6eec90-6e9b-48ea-d268-9b28ce041272"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "6DCYjuw0ja8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code defines a function `create_kaggle_dir()` to create a directory named \".kaggle\" in the root directory \"/root/\" if it does not exist. \n",
        "\n",
        "The next function, `retrieve_kaggle_json()`, retrieves the kaggle.json file from a specified source directory and places it in the \".kaggle\" directory. The file is used for authentication when using the Kaggle API. The function sets appropriate file permissions using chmod.\n",
        "\n",
        "The next two functions, `download_facemask_dataset()` and `unzip_facemask_dataset()`, download and unzip a face mask detection dataset respectively if it does not already exist in the \"/content/\" directory.\n",
        "\n",
        "The final function, `dataset_pipeline()`, calls all the above functions in the following order - `create_kaggle_dir()`, `retrieve_kaggle_json()`, `download_facemask_dataset()`, and `unzip_facemask_dataset()`.\n",
        "\n",
        "The last line of the code executes the `dataset_pipeline()` function. This code can be useful for setting up the required dataset for a machine learning project related to face mask detection."
      ],
      "metadata": {
        "id": "VhcDIbPUJbfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_kaggle_dir(kaggle_dir=\"/root/.kaggle/\"):\n",
        "    if not os.path.exists(kaggle_dir):\n",
        "        !mkdir $kaggle_dir\n",
        "\n",
        "def retrieve_kaggle_json(source_dir=\"/content/drive/MyDrive/\", kaggle_dir=\"/root/.kaggle/\", file_name=\"kaggle.json\"):\n",
        "    source_file = os.path.join(source_dir, file_name)\n",
        "    target_file = os.path.join(kaggle_dir, file_name)\n",
        "\n",
        "    if not os.path.exists(target_file):\n",
        "        !cp $source_file $kaggle_dir\n",
        "        !chmod 600 $target_file\n",
        "\n",
        "def download_facemask_dataset():\n",
        "    if not os.path.exists(\"/content/face-mask-detection.zip\"):\n",
        "        !kaggle datasets download -d dhruvmak/face-mask-detection\n",
        "\n",
        "def unzip_facemask_dataset():\n",
        "    if not os.path.exists(\"/content/face-mask-detection\"):\n",
        "        !unzip face-mask-detection.zip -d face-mask-detection > /dev/null 2>&1\n",
        "\n",
        "def dataset_pipeline():\n",
        "    create_kaggle_dir()\n",
        "    retrieve_kaggle_json()\n",
        "    download_facemask_dataset()\n",
        "    unzip_facemask_dataset()\n",
        "\n",
        "dataset_pipeline()"
      ],
      "metadata": {
        "id": "ENgYkyHdRxin"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 224\n",
        "\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255.0, \n",
        "                                                          shear_range=0.2, \n",
        "                                                          zoom_range=0.2, \n",
        "                                                          rotation_range=45, \n",
        "                                                          horizontal_flip=True, \n",
        "                                                          validation_split=0.2)"
      ],
      "metadata": {
        "id": "gpFU0SzmTrXA"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code uses the Keras `ImageDataGenerator` class to create two data generators named `train` and `test`. \n",
        "\n",
        "The `flow_from_directory()` method of the `ImageDataGenerator` class is used to load the training and validation datasets from the specified directory '/content/face-mask-detection/dataset/'. The images in the directory are resized to the specified `IMG_SIZE` and augmented by the `datagen` object. \n",
        "\n",
        "Both the training and validation datasets are loaded as batches of 16 images at a time, with the `class_mode` argument set to 'binary' as there are only two classes (with mask and without mask) in the dataset. The `interpolation` argument is set to 'lanczos' for better image quality. \n",
        "\n",
        "The `subset` argument is set to 'training' for the `train` data generator and 'validation' for the `test` data generator to specify the type of dataset to be loaded. Finally, the `shuffle` argument is set to `True` to shuffle the order of the images during training and testing."
      ],
      "metadata": {
        "id": "EeDCopmfKFLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = datagen.flow_from_directory('/content/face-mask-detection/dataset/', target_size=(IMG_SIZE,IMG_SIZE), batch_size=16, \n",
        "                                    class_mode='binary', interpolation='lanczos', shuffle=True, subset='training')\n",
        "\n",
        "test = datagen.flow_from_directory('/content/face-mask-detection/dataset/', target_size=(IMG_SIZE,IMG_SIZE), batch_size=16, \n",
        "                                    class_mode='binary', interpolation='lanczos', shuffle=True, subset='validation')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9316gthT8zK",
        "outputId": "534df4dd-3eaa-4224-e0d5-4101e6168250"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 352 images belonging to 2 classes.\n",
            "Found 88 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Creation"
      ],
      "metadata": {
        "id": "YOSOJhFRUwSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code defines a deep learning model using the Keras `Sequential` API. \n",
        "\n",
        "The model architecture consists of several layers, including a pre-trained `MobileNet` layer, followed by a `Conv2D` layer, a `Flatten` layer, and a `Dense` layer. \n",
        "\n",
        "The input shape of the model is specified using the `Input` layer. The `IMG_SIZE` is used to set the height and width of the input image, and `3` specifies the number of color channels.\n",
        "\n",
        "The model is compiled using the `compile()` method, which specifies the loss function, optimizer, and evaluation metric. The `binary_crossentropy` loss function is used since the dataset has only two classes. The `rmsprop` optimizer is used to minimize the loss function during training, and `accuracy` is used as the evaluation metric.\n",
        "\n",
        "Finally, the `summary()` method is used to display the model summary, which shows the layer-wise structure of the model and the number of parameters in each layer."
      ],
      "metadata": {
        "id": "YHvXo9cFKdTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/Lab-Proj/models/mobilenet-facemask2.h5\"\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    print(\"Loading existing model . . .\")\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "else:\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Input((IMG_SIZE,IMG_SIZE,3)),\n",
        "        tf.keras.applications.MobileNet(include_top=False, weights='imagenet'),\n",
        "        tf.keras.layers.Conv2D(3, 3, padding='same', activation='sigmoid'),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "    model.fit(train, epochs=5, validation_data=test)\n",
        "\n",
        "    model.save(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gc6tim1aUZVZ",
        "outputId": "920055f5-5f60-43fd-f675-30361a992aa9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "22/22 [==============================] - 27s 712ms/step - loss: 0.2794 - accuracy: 0.8778 - val_loss: 0.3178 - val_accuracy: 0.8864\n",
            "Epoch 2/5\n",
            "22/22 [==============================] - 15s 687ms/step - loss: 0.1057 - accuracy: 0.9716 - val_loss: 0.1822 - val_accuracy: 0.9886\n",
            "Epoch 3/5\n",
            "22/22 [==============================] - 17s 809ms/step - loss: 0.0742 - accuracy: 0.9830 - val_loss: 0.1075 - val_accuracy: 0.9773\n",
            "Epoch 4/5\n",
            "22/22 [==============================] - 16s 746ms/step - loss: 0.0508 - accuracy: 0.9830 - val_loss: 0.0707 - val_accuracy: 0.9886\n",
            "Epoch 5/5\n",
            "22/22 [==============================] - 15s 680ms/step - loss: 0.0250 - accuracy: 0.9972 - val_loss: 0.0540 - val_accuracy: 0.9886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMIpWmVMO7CR",
        "outputId": "8293cf71-7f25-4d0e-b3fc-9428b85a04ae"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " mobilenet_1.00_224 (Functio  (None, None, None, 1024)  3228864  \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 7, 7, 3)           27651     \n",
            "                                                                 \n",
            " flatten_17 (Flatten)        (None, 147)               0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 1)                 148       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,256,663\n",
            "Trainable params: 3,234,775\n",
            "Non-trainable params: 21,888\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u71FKhGX5s1u",
        "outputId": "3c2c2bea-0653-457f-ac41-b8e3add47db3"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 3s 435ms/step - loss: 0.0340 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.034043919295072556, 1.0]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict Custom Images"
      ],
      "metadata": {
        "id": "81YOmTfP-RCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code defines a function named `predict` that takes a file path as input.\n",
        "\n",
        "The function loads an image from the specified file path using the `load_img` method of `keras.preprocessing.image`. The image is then resized to the specified `IMG_SIZE` and converted to a NumPy array using the `img_to_array` method. \n",
        "\n",
        "The pixel values of the image are normalized by dividing with 255.0. The model is then used to predict whether the image contains a person wearing a mask or not, by passing the processed image as input to the `predict` method of the `model` object. \n",
        "\n",
        "The predicted result is stored in the `pred` variable as a probability value. The image is then plotted using `matplotlib.pyplot.imshow` and a title is added to the plot using `plt.title`. The title displays the predicted class and the corresponding probability score in percentage.\n",
        "\n",
        "If the predicted probability value is greater than 0.5, the image is classified as 'Without Mask' and if it is less than or equal to 0.5, the image is classified as 'With Mask'."
      ],
      "metadata": {
        "id": "zS-Y31VJKwqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(path) :\n",
        "    img = tf.keras.preprocessing.image.load_img(path, target_size=(IMG_SIZE,IMG_SIZE), interpolation='lanczos')\n",
        "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img = img / 255.0\n",
        "    pred = model.predict(np.array([img]))\n",
        "    plt.imshow(img)\n",
        "    if pred[0][0] > 0.5 : \n",
        "        plt.title(f'Without Mask : {pred[0][0]*100 : 0.2f}%')\n",
        "    else : \n",
        "        plt.title(f'With Mask : {(1-pred[0][0])*100 : 0.2f}%')"
      ],
      "metadata": {
        "id": "0Xz537lM-RX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Lab-Proj/testing/no-mask-3.jpeg\"\n",
        "predict(path)"
      ],
      "metadata": {
        "id": "i_D9ADQdGhVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models Variants"
      ],
      "metadata": {
        "id": "dRvsYDv1gZea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 224\n",
        "\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(validation_split=0.2)"
      ],
      "metadata": {
        "id": "kIZijmSC_HZc"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code uses the Keras `ImageDataGenerator` class to create two data generators named `train` and `test`. \n",
        "\n",
        "The `flow_from_directory()` method of the `ImageDataGenerator` class is used to load the training and validation datasets from the specified directory '/content/face-mask-detection/dataset/'. The images in the directory are resized to the specified `IMG_SIZE` and augmented by the `datagen` object. \n",
        "\n",
        "Both the training and validation datasets are loaded as batches of 16 images at a time, with the `class_mode` argument set to 'binary' as there are only two classes (with mask and without mask) in the dataset. The `interpolation` argument is set to 'lanczos' for better image quality. \n",
        "\n",
        "The `subset` argument is set to 'training' for the `train` data generator and 'validation' for the `test` data generator to specify the type of dataset to be loaded. Finally, the `shuffle` argument is set to `True` to shuffle the order of the images during training and testing."
      ],
      "metadata": {
        "id": "t7eWZZcB_HZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = datagen.flow_from_directory('/content/face-mask-detection/dataset/', target_size=(IMG_SIZE,IMG_SIZE), batch_size=16, \n",
        "                                    class_mode='binary', interpolation='lanczos', shuffle=True, subset='training')\n",
        "\n",
        "test = datagen.flow_from_directory('/content/face-mask-detection/dataset/', target_size=(IMG_SIZE,IMG_SIZE), batch_size=16, \n",
        "                                    class_mode='binary', interpolation='lanczos', shuffle=False, subset='validation')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33065265-d5a6-4b1e-9d1a-f62575601bde",
        "id": "I1LGfdpf_HZ1"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 352 images belonging to 2 classes.\n",
            "Found 88 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code defines a deep learning model using the Keras `Sequential` API, defining an heavier and more performing variant of the previous `MobileNet` model. \n",
        "\n",
        "The model architecture consists of several layers, including a pre-trained `ResNet50` layer, followed by a `Conv2D` layer, a `Flatten` layer, and a `Dense` layer. \n",
        "\n",
        "The input shape of the model is specified using the `Input` layer. The `IMG_SIZE` is used to set the height and width of the input image, and `3` specifies the number of color channels.\n",
        "\n",
        "The model is compiled using the `compile()` method, which specifies the loss function, optimizer, and evaluation metric. The `binary_crossentropy` loss function is used since the dataset has only two classes. The `rmsprop` optimizer is used to minimize the loss function during training, and `accuracy` is used as the evaluation metric.\n",
        "\n",
        "Finally, the `summary()` method is used to display the model summary, which shows the layer-wise structure of the model and the number of parameters in each layer."
      ],
      "metadata": {
        "id": "yqWDzTQhgy_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/Lab-Proj/models/resnet-facemask.h5\"\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    print(\"Loading existing model . . .\")\n",
        "    model = keras.models.load_model(model_path)\n",
        "\n",
        "else:\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Input((IMG_SIZE,IMG_SIZE,3)),\n",
        "        tf.keras.applications.ResNet50(include_top=False, weights='imagenet'),\n",
        "        tf.keras.layers.Conv2D(3, 3, padding='same', activation='sigmoid'),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "    \n",
        "    model.fit(train, epochs=50, validation_data=test)\n",
        "    model.save(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ef0bf09-18b9-4387-a44b-28ea6385a9e8",
        "id": "Ixw11n20gy_K"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "22/22 [==============================] - 43s 593ms/step - loss: 0.3193 - accuracy: 0.8750 - val_loss: 0.9493 - val_accuracy: 0.5000\n",
            "Epoch 2/50\n",
            "22/22 [==============================] - 11s 479ms/step - loss: 0.1066 - accuracy: 0.9716 - val_loss: 1.5043 - val_accuracy: 0.5000\n",
            "Epoch 3/50\n",
            "22/22 [==============================] - 12s 519ms/step - loss: 0.0332 - accuracy: 0.9972 - val_loss: 1.3728 - val_accuracy: 0.5000\n",
            "Epoch 4/50\n",
            "22/22 [==============================] - 12s 526ms/step - loss: 0.0747 - accuracy: 0.9688 - val_loss: 0.7381 - val_accuracy: 0.5795\n",
            "Epoch 5/50\n",
            "22/22 [==============================] - 12s 532ms/step - loss: 0.0642 - accuracy: 0.9858 - val_loss: 0.1560 - val_accuracy: 0.9432\n",
            "Epoch 6/50\n",
            "22/22 [==============================] - 12s 515ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.0773 - val_accuracy: 0.9773\n",
            "Epoch 7/50\n",
            "22/22 [==============================] - 11s 524ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0486 - val_accuracy: 0.9886\n",
            "Epoch 8/50\n",
            "22/22 [==============================] - 11s 520ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0466 - val_accuracy: 0.9886\n",
            "Epoch 9/50\n",
            "22/22 [==============================] - 12s 504ms/step - loss: 9.4241e-04 - accuracy: 1.0000 - val_loss: 0.0465 - val_accuracy: 0.9886\n",
            "Epoch 10/50\n",
            "22/22 [==============================] - 11s 501ms/step - loss: 5.2313e-04 - accuracy: 1.0000 - val_loss: 0.0494 - val_accuracy: 0.9886\n",
            "Epoch 11/50\n",
            "22/22 [==============================] - 12s 532ms/step - loss: 3.1943e-04 - accuracy: 1.0000 - val_loss: 0.0512 - val_accuracy: 0.9886\n",
            "Epoch 12/50\n",
            "22/22 [==============================] - 12s 520ms/step - loss: 2.2466e-04 - accuracy: 1.0000 - val_loss: 0.0511 - val_accuracy: 0.9886\n",
            "Epoch 13/50\n",
            "22/22 [==============================] - 12s 529ms/step - loss: 1.7476e-04 - accuracy: 1.0000 - val_loss: 0.0515 - val_accuracy: 0.9886\n",
            "Epoch 14/50\n",
            "22/22 [==============================] - 12s 524ms/step - loss: 1.3156e-04 - accuracy: 1.0000 - val_loss: 0.0523 - val_accuracy: 0.9886\n",
            "Epoch 15/50\n",
            "22/22 [==============================] - 12s 525ms/step - loss: 1.1299e-04 - accuracy: 1.0000 - val_loss: 0.0522 - val_accuracy: 0.9886\n",
            "Epoch 16/50\n",
            "22/22 [==============================] - 14s 635ms/step - loss: 9.2604e-05 - accuracy: 1.0000 - val_loss: 0.0531 - val_accuracy: 0.9886\n",
            "Epoch 17/50\n",
            "22/22 [==============================] - 12s 523ms/step - loss: 7.8856e-05 - accuracy: 1.0000 - val_loss: 0.0524 - val_accuracy: 0.9886\n",
            "Epoch 18/50\n",
            "22/22 [==============================] - 14s 631ms/step - loss: 7.2401e-05 - accuracy: 1.0000 - val_loss: 0.0529 - val_accuracy: 0.9886\n",
            "Epoch 19/50\n",
            "22/22 [==============================] - 12s 520ms/step - loss: 6.0322e-05 - accuracy: 1.0000 - val_loss: 0.0530 - val_accuracy: 0.9886\n",
            "Epoch 20/50\n",
            "22/22 [==============================] - 14s 617ms/step - loss: 6.2433e-05 - accuracy: 1.0000 - val_loss: 0.0521 - val_accuracy: 0.9886\n",
            "Epoch 21/50\n",
            "22/22 [==============================] - 12s 524ms/step - loss: 5.5993e-05 - accuracy: 1.0000 - val_loss: 0.0518 - val_accuracy: 0.9886\n",
            "Epoch 22/50\n",
            "22/22 [==============================] - 14s 638ms/step - loss: 5.0710e-05 - accuracy: 1.0000 - val_loss: 0.0536 - val_accuracy: 0.9886\n",
            "Epoch 23/50\n",
            "22/22 [==============================] - 12s 515ms/step - loss: 4.3361e-05 - accuracy: 1.0000 - val_loss: 0.0541 - val_accuracy: 0.9886\n",
            "Epoch 24/50\n",
            "22/22 [==============================] - 12s 529ms/step - loss: 4.6355e-05 - accuracy: 1.0000 - val_loss: 0.0538 - val_accuracy: 0.9886\n",
            "Epoch 25/50\n",
            "22/22 [==============================] - 12s 543ms/step - loss: 4.0307e-05 - accuracy: 1.0000 - val_loss: 0.0542 - val_accuracy: 0.9886\n",
            "Epoch 26/50\n",
            "22/22 [==============================] - 12s 543ms/step - loss: 3.9957e-05 - accuracy: 1.0000 - val_loss: 0.0541 - val_accuracy: 0.9886\n",
            "Epoch 27/50\n",
            "22/22 [==============================] - 12s 544ms/step - loss: 3.2977e-05 - accuracy: 1.0000 - val_loss: 0.0551 - val_accuracy: 0.9886\n",
            "Epoch 28/50\n",
            "22/22 [==============================] - 12s 545ms/step - loss: 3.3330e-05 - accuracy: 1.0000 - val_loss: 0.0553 - val_accuracy: 0.9886\n",
            "Epoch 29/50\n",
            "22/22 [==============================] - 12s 540ms/step - loss: 3.3877e-05 - accuracy: 1.0000 - val_loss: 0.0551 - val_accuracy: 0.9886\n",
            "Epoch 30/50\n",
            "22/22 [==============================] - 12s 543ms/step - loss: 3.0359e-05 - accuracy: 1.0000 - val_loss: 0.0568 - val_accuracy: 0.9886\n",
            "Epoch 31/50\n",
            "22/22 [==============================] - 12s 542ms/step - loss: 2.6595e-05 - accuracy: 1.0000 - val_loss: 0.0574 - val_accuracy: 0.9886\n",
            "Epoch 32/50\n",
            "22/22 [==============================] - 12s 516ms/step - loss: 2.6975e-05 - accuracy: 1.0000 - val_loss: 0.0583 - val_accuracy: 0.9886\n",
            "Epoch 33/50\n",
            "22/22 [==============================] - 11s 506ms/step - loss: 2.5471e-05 - accuracy: 1.0000 - val_loss: 0.0594 - val_accuracy: 0.9886\n",
            "Epoch 34/50\n",
            "22/22 [==============================] - 12s 514ms/step - loss: 3.7225e-05 - accuracy: 1.0000 - val_loss: 0.0624 - val_accuracy: 0.9886\n",
            "Epoch 35/50\n",
            "22/22 [==============================] - 11s 506ms/step - loss: 2.2808e-05 - accuracy: 1.0000 - val_loss: 0.0624 - val_accuracy: 0.9886\n",
            "Epoch 36/50\n",
            "22/22 [==============================] - 12s 529ms/step - loss: 2.1550e-05 - accuracy: 1.0000 - val_loss: 0.0619 - val_accuracy: 0.9886\n",
            "Epoch 37/50\n",
            "22/22 [==============================] - 12s 519ms/step - loss: 5.1355e-05 - accuracy: 1.0000 - val_loss: 0.0343 - val_accuracy: 0.9886\n",
            "Epoch 38/50\n",
            "22/22 [==============================] - 12s 537ms/step - loss: 2.3520e-05 - accuracy: 1.0000 - val_loss: 0.0391 - val_accuracy: 0.9886\n",
            "Epoch 39/50\n",
            "22/22 [==============================] - 12s 532ms/step - loss: 2.7894e-05 - accuracy: 1.0000 - val_loss: 0.0418 - val_accuracy: 0.9886\n",
            "Epoch 40/50\n",
            "22/22 [==============================] - 11s 488ms/step - loss: 2.1734e-05 - accuracy: 1.0000 - val_loss: 0.0475 - val_accuracy: 0.9886\n",
            "Epoch 41/50\n",
            "22/22 [==============================] - 12s 517ms/step - loss: 2.1988e-05 - accuracy: 1.0000 - val_loss: 0.0517 - val_accuracy: 0.9886\n",
            "Epoch 42/50\n",
            "22/22 [==============================] - 12s 526ms/step - loss: 1.8573e-05 - accuracy: 1.0000 - val_loss: 0.0546 - val_accuracy: 0.9886\n",
            "Epoch 43/50\n",
            "22/22 [==============================] - 12s 523ms/step - loss: 1.8060e-05 - accuracy: 1.0000 - val_loss: 0.0584 - val_accuracy: 0.9886\n",
            "Epoch 44/50\n",
            "22/22 [==============================] - 12s 522ms/step - loss: 1.8031e-05 - accuracy: 1.0000 - val_loss: 0.0615 - val_accuracy: 0.9886\n",
            "Epoch 45/50\n",
            "22/22 [==============================] - 12s 520ms/step - loss: 1.5320e-05 - accuracy: 1.0000 - val_loss: 0.0640 - val_accuracy: 0.9886\n",
            "Epoch 46/50\n",
            "22/22 [==============================] - 11s 521ms/step - loss: 1.6945e-05 - accuracy: 1.0000 - val_loss: 0.0661 - val_accuracy: 0.9886\n",
            "Epoch 47/50\n",
            "22/22 [==============================] - 12s 523ms/step - loss: 1.6000e-05 - accuracy: 1.0000 - val_loss: 0.0670 - val_accuracy: 0.9886\n",
            "Epoch 48/50\n",
            "22/22 [==============================] - 11s 517ms/step - loss: 1.5280e-05 - accuracy: 1.0000 - val_loss: 0.0675 - val_accuracy: 0.9886\n",
            "Epoch 49/50\n",
            "22/22 [==============================] - 11s 514ms/step - loss: 2.1410e-05 - accuracy: 1.0000 - val_loss: 0.0684 - val_accuracy: 0.9886\n",
            "Epoch 50/50\n",
            "22/22 [==============================] - 11s 470ms/step - loss: 3.0238e-05 - accuracy: 1.0000 - val_loss: 0.0478 - val_accuracy: 0.9886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10e5297c-6bb0-4697-cc49-23ad7da8336f",
        "id": "90Zljg1Ugy_M"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_30\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet50 (Functional)       (None, None, None, 2048)  23587712  \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 7, 7, 3)           55299     \n",
            "                                                                 \n",
            " flatten_24 (Flatten)        (None, 147)               0         \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 1)                 148       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,643,159\n",
            "Trainable params: 23,590,039\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0eb611d-d537-4a5d-97f7-e761ac524c99",
        "id": "K1rmq7qZgy_N"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 2s 258ms/step - loss: 0.0478 - accuracy: 0.9886\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.04780176654458046, 0.9886363744735718]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    }
  ]
}